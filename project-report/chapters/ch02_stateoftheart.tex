%!TEX root = ../report.tex

\begin{document}
    \chapter{State of the Art}
	There are several ways to obtain faster inference for DNN. One of the method is model compression. Compressing models of DNN can be divided into three major classes: \cite{Wen2017LearningIS} removal of DNN structure that is redundant, approximation of DNN function, and architecture search which creates and designs a compact DNN. This research is concentrated on the first two methods.
	
	Model compression by removal of redundant structures, or pruning, are extensively studied. Pruning the network is an example that is widely used as it is able to simplify DNN models while also retain the performance of the original models. \cite{Tang2019BringingGN}
	
	Knowledge distillation, on the other hand, is an example of a method that belongs in approximating DNN function. A large computationally expensive model is able to be compressed into a single computational efficient neural network. \cite{Mirzadeh2019ImprovedKD}.
	
	In this section,each method will be introduced and then explained how it is used in the experiments on section 4. 
	
    \section{Pruning Methods}
    Pruning in neural network field is an act to reduce the extent of the network by removal of superfluous or unwanted parts. Pruning methods are divided into two main categories: unstructured pruning (fine-grained pruning) and structured pruning (coarse-grained pruning) \cite{8114708}
    
    \subsection{Unstructured Pruning}
    Unstructured or fine-grained pruning is a method to eliminate weight parameters of the neural network that are deemed unnecessary. As the result network will be sparse, specialized hardware or libraries will be needed in order to fasten the inference of the network.\cite{Liu2018RethinkingTV} 
    
    Several works developed in this methods are individual weight pruning based on Hessian matrix, \cite{NIPS1992_647} deep compression by training, pruning, and fine tuning \cite{Han2015DeepCC}, and variational dropout \cite{Molchanov2017VariationalDS}
    
    \subsection{Structured Pruning}
    Structured pruning or coarse-grained pruning is a method to eliminate channels or layers of DNN. Channels or layers are determined through the importance either globally (automatic pruning) or locally. Structured pruning is delved more as it requires no specialized hardware or software. \cite{NIPS2016_6504}. 
    
    In line to structured pruning methods, In \cite{Anwar:2017:SPD:3051701.3005348}, Patricle filter is used to obtain smaller DNN. 
    
    \section{Quantization Method}
    \section{Knowledge Distillation Methods}
    Deep Neural Network (DNN) operates with long inference time as the network is deep and contains a lot of parameters. Knowledge distillation \cite{Hinton2015DistillingTK} aims to reduce this by creating a student network where it consists of less number of parameters and less depth that learns from a teacher, in this case, is a DNN.
    
    There are two main points that it differs from transfer learning, the first point is that the model architecture in transfer learning is similar with shared layers, while in knowledge distillation (KD) the architecture between the students and the teacher are not the same. The second point is that in transfer learning, it copies all the weights of a DNN, while in KD it only imitates the Teacher.
    
    Therefore, in this learning method, the student is learning from a Dark Knowledge 
    \section{Limitations of previous work}
\end{document}
