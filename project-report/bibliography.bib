% This file was created with JabRef 2.10.
% Encoding: UTF-8


@Article{Anwar:2017:SPD:3051701.3005348,
  Title                    = {Structured Pruning of Deep Convolutional Neural Networks},
  Author                   = {Anwar, Sajid and Hwang, Kyuyeon and Sung, Wonyong},
  Journal                  = {J. Emerg. Technol. Comput. Syst.},
  Year                     = {2017},

  Month                    = feb,
  Number                   = {3},
  Pages                    = {32:1--32:18},
  Volume                   = {13},

  Acmid                    = {3005348},
  Address                  = {New York, NY, USA},
  Articleno                = {32},
  Doi                      = {10.1145/3005348},
  ISSN                     = {1550-4832},
  Issue_date               = {May 2017},
  Keywords                 = {Deep convolutional neural networks, feature map pruning, intra-kernel strided sparsity, structured pruning},
  Numpages                 = {18},
  Publisher                = {ACM},
  Url                      = {http://doi.acm.org/10.1145/3005348}
}

@Article{Han2015DeepCC,
  Title                    = {Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding},
  Author                   = {Song Han and Huizi Mao and William J. Dally},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1510.00149}
}

@InCollection{NIPS1992_647,
  Title                    = {Second order derivatives for network pruning: Optimal Brain Surgeon},
  Author                   = {Hassibi, Babak and David G. Stork},
  Booktitle                = {Advances in Neural Information Processing Systems 5},
  Publisher                = {Morgan-Kaufmann},
  Year                     = {1993},
  Editor                   = {S. J. Hanson and J. D. Cowan and C. L. Giles},
  Pages                    = {164--171},

  Owner                    = {diennur},
  Timestamp                = {2019.10.16},
  Url                      = {http://papers.nips.cc/paper/647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf}
}

@InProceedings{10.1007/978-3-319-46493-0_38,
  Title                    = {Identity Mappings in Deep Residual Networks},
  Author                   = {He, Kaiming
and Zhang, Xiangyu
and Ren, Shaoqing
and Sun, Jian},
  Booktitle                = {Computer Vision -- ECCV 2016},
  Year                     = {2016},

  Address                  = {Cham},
  Editor                   = {Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max},
  Pages                    = {630--645},
  Publisher                = {Springer International Publishing},

  Abstract                 = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62Â {\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.},
  ISBN                     = {978-3-319-46493-0},
  Owner                    = {diennur},
  Timestamp                = {2019.10.25}
}

@Article{Hinton2015DistillingTK,
  Title                    = {Distilling the Knowledge in a Neural Network},
  Author                   = {Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  Journal                  = {ArXiv},
  Year                     = {2015},
  Volume                   = {abs/1503.02531}
}

@TechReport{Krizhevsky09learningmultiple,
  Title                    = {Learning multiple layers of features from tiny images},
  Author                   = {Alex Krizhevsky},
  Year                     = {2009}
}

@Article{Li2016PruningFF,
  Title                    = {Pruning Filters for Efficient ConvNets},
  Author                   = {Hao Li and Asim Kadav and Igor Durdanovic and Hanan Samet and Hans Peter Graf},
  Journal                  = {ArXiv},
  Year                     = {2016},
  Volume                   = {abs/1608.08710}
}

@InProceedings{Liu_2017_ICCV,
  Title                    = {Learning Efficient Convolutional Networks Through Network Slimming},
  Author                   = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  Booktitle                = {The IEEE International Conference on Computer Vision (ICCV)},
  Year                     = {2017},
  Month                    = {Oct}
}

@Article{Liu2018RethinkingTV,
  Title                    = {Rethinking the Value of Network Pruning},
  Author                   = {Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
  Journal                  = {ArXiv},
  Year                     = {2018},
  Volume                   = {abs/1810.05270}
}

@Article{Mirzadeh2019ImprovedKD,
  Title                    = {Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher},
  Author                   = {Seyed-Iman Mirzadeh and Mehrdad Farajtabar and Ang Li and Hassan Ghasemzadeh},
  Journal                  = {ArXiv},
  Year                     = {2019},
  Volume                   = {abs/1902.03393}
}

@InProceedings{Molchanov2017VariationalDS,
  Title                    = {Variational Dropout Sparsifies Deep Neural Networks},
  Author                   = {Dmitry Molchanov and Arsenii Ashukha and Dmitry P. Vetrov},
  Booktitle                = {ICML},
  Year                     = {2017}
}

@Article{art1,
  Title                    = {Book Title},
  Author                   = {Author Name},
  Journal                  = {Lecture Notes in Autonomous System},
  Year                     = {2003},
  Pages                    = {900--921},
  Volume                   = {1001},

  Bibdate                  = {Sat Dec 7 10:05:42 MST 2003},
  Coden                    = {LNCSD9},
  ISSN                     = {0302-2345},
  Publisher                = {UFO}
}

@Article{8114708,
  Title                    = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
  Author                   = {V. {Sze} and Y. {Chen} and T. {Yang} and J. S. {Emer}},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {2017},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {2295-2329},
  Volume                   = {105},

  Abstract                 = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
  Doi                      = {10.1109/JPROC.2017.2761740},
  Keywords                 = {artificial intelligence;computational complexity;neural nets;energy efficiency;hardware design changes;DNN hardware designs;deep neural networks;hardware cost;computation cost reduction;artificial intelligence;computational complexity;hardware platforms;hardware architecture;DNN hardware implementations;Neurons;Biological neural networks;Artificial intelligence;Machine learning;Neural networks;Tutorials;Convolutional neural networks;Artificial intelligence;Benchmark testing;Computer architecture;ASIC;computer architecture;convolutional neural networks;dataflow processing;deep learning;deep neural networks;energy-efficient accelerators;low power;machine learning;spatial architectures;VLSI}
}

@Article{Tang2019BringingGN,
  Title                    = {Bringing Giant Neural Networks Down to Earth with Unlabeled Data},
  Author                   = {Yehui Tang and Shan You and Chang Xu and Boxin Shi and Chao Xu},
  Journal                  = {ArXiv},
  Year                     = {2019},
  Volume                   = {abs/1907.06065}
}

@Article{Wen2017LearningIS,
  Title                    = {Learning Intrinsic Sparse Structures within Long Short-Term Memory},
  Author                   = {Wei Wen and Yuxiong He and Samyam Rajbhandari and Minjia Zhang and Wenhan Wang and Fang Liu and Bin Hu and Yiran Chen and Hai Li},
  Journal                  = {ArXiv},
  Year                     = {2017},
  Volume                   = {abs/1709.05027}
}

@InCollection{NIPS2016_6504,
  Title                    = {Learning Structured Sparsity in Deep Neural Networks},
  Author                   = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  Booktitle                = {Advances in Neural Information Processing Systems 29},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2016},
  Editor                   = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  Pages                    = {2074--2082},

  Owner                    = {diennur},
  Timestamp                = {2019.10.25},
  Url                      = {http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf}
}

