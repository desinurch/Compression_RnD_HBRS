# Research and Development Project HBRS
## A Comparative Study of Sparsity Methods in Deep Neural Network for Faster Inference

Code and documentation for research and development project with a topic in Deep Neural Network Compression as partial fulfillmrnt in Masters of Autonomous Systems program.

### Repository structure:
	- L1 norm pruning: clone of https://github.com/Eric-mingjie/rethinking-network-pruning/tree/master/cifar/l1-norm-pruning based on the implementation of the paper "[Pruning Filters For Efficient ConvNets][https://arxiv.org/pdf/1608.08710.pdf]"
	- NetAdapt: clone of https://github.com/NatGr/Master_Thesis/tree/master/NetAdapt implementation of the paper "[NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications][http://arxiv.org/abs/1804.03230]", several modifications were made (the main ones are: fisher pruning instead of weights-norm pruning and training from scratch rather than long term fine-tuning)
	- report of research and development project
